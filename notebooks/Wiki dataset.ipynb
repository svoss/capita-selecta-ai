{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki dataset\n",
    "Provides functionallity that extract word sequences from wikimedia dumps: https://dumps.wikimedia.org/backup-index.html. The result consists of a long list of integers representing the word sequence and a dictionary that maps these integers to actual words. \n",
    "\n",
    "To do so it makes use of the wikiextractor(https://github.com/attardi/wikiextractor/)  package that can extract text from wikidumps. The functionallity is build on top of the provided chainer dataset functions(that provide caching etc.) and inspired by https://github.com/pfnet/chainer/blob/master/chainer/datasets/ptb.py .\n",
    "\n",
    "This code can also be imported as a script from code/wiki_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#General depencies\n",
    "import sys\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install wikiextractor\n",
    "In order to extract our dump and extract the text we make use of wiki extractor, the following function downloads it and appends it to the system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROOT = os.path.dirname(os.getcwd())\n",
    "GITHUB_ZIP = \"https://github.com/attardi/wikiextractor/archive/master.zip\"\n",
    "def install_wiki_extractor():\n",
    "    #if wikiextractor folder does not exist \n",
    "    wiki_extractor_path = os.path.join(ROOT,'wikiextractor')\n",
    "    if not os.path.isdir(wiki_extractor_path):\n",
    "        #download zip from repo\n",
    "        print \"Downloading wiki extractor\"\n",
    "        r = requests.get(GITHUB_ZIP)\n",
    "        zip_path = os.path.join(ROOT,'wikiextractor.zip') \n",
    "        with open(zip_path, \"wb\") as code:\n",
    "            code.write(r.content)\n",
    "        \n",
    "        #extract zip from zip\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(ROOT)\n",
    "        \n",
    "        #by default the zip is extract to ROOT/wikiextractor-master we move this to the ROOT/wikiextractor path\n",
    "        shutil.move(os.path.join(ROOT,'wikiextractor-master'),wiki_extractor_path)\n",
    "\n",
    "        #remove zip file \n",
    "        os.remove(zip_path)\n",
    "        #append to path\n",
    "    sys.path.append(wiki_extractor_path)\n",
    "install_wiki_extractor()\n",
    "#very ugly hack, to make discardElements global work\n",
    "import __builtin__\n",
    "__builtin__.discardElements = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import hashlib\n",
    "import fileinput\n",
    "from chainer.dataset import download\n",
    "import tempfile\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wiki extractor functionality works as a script with arguments by default. To be able to call it from another function I re-implement the main function but with function arguments instead of script arguments. It takes a bz2 dump file as input and outputs a list of folders (AA-ZZ) with at most 100 files per folder. These files then contain the text from different docs. Defined in the <doc> </doc> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from WikiExtractor import Extractor, minFileSize, ignoreTag, load_templates, pages_from,process_dump,cpu_count,filter_disambig_pages\n",
    "\n",
    "# function extracts templates from dump, it uses the WikiExtractor module to do so\n",
    "# Based on the main() function in WikiExtractor.py, but replaces params\n",
    "def extract_dump(input, output=None, bytes=\"1M\", compress=False, html=False, links=False, sections=False, lists=False,\n",
    "                 namespaces=False, templates=False, no_templates=True, revision=False,\n",
    "                 min_text_length=Extractor.min_text_length, arg_filter_disambig_pages=False, processes=False, quiet=False,\n",
    "                 debug=False, article=False, version=\"\"):\n",
    "    global urlbase, acceptedNamespaces, filter_disambig_pages\n",
    "    global templateCache\n",
    "\n",
    "    default_process_count = cpu_count() - 1\n",
    "    if processes is False:\n",
    "        processes = default_process_count\n",
    "\n",
    "    Extractor.keepLinks = links\n",
    "    Extractor.keepSections = sections\n",
    "    Extractor.keepLists = lists\n",
    "    Extractor.toHTML = html\n",
    "    Extractor.print_revision = revision\n",
    "    Extractor.min_text_length = min_text_length\n",
    "    if html:\n",
    "        Extractor.keepLinks = True\n",
    "\n",
    "    Extractor.expand_templates = no_templates\n",
    "    filter_disambig_pages = arg_filter_disambig_pages\n",
    "\n",
    "    try:\n",
    "        power = 'kmg'.find(bytes[-1].lower()) + 1\n",
    "        file_size = int(bytes[:-1]) * 1024 ** power\n",
    "        if file_size < minFileSize:\n",
    "            raise ValueError()\n",
    "    except ValueError:\n",
    "        logging.error('Insufficient or invalid size: %s', bytes)\n",
    "        return\n",
    "\n",
    "    if namespaces:\n",
    "        acceptedNamespaces = set(namespaces.split(','))\n",
    "\n",
    "    FORMAT = '%(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=FORMAT)\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    if not quiet:\n",
    "        logger.setLevel(logging.INFO)\n",
    "    if debug:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    input_file = input\n",
    "\n",
    "    if not Extractor.keepLinks:\n",
    "        ignoreTag('a')\n",
    "\n",
    "    # sharing cache of parser templates is too slow:\n",
    "    # manager = Manager()\n",
    "    # templateCache = manager.dict()\n",
    "\n",
    "    if article:\n",
    "        if templates:\n",
    "            if os.path.exists(templates):\n",
    "                with open(templates) as file:\n",
    "                    load_templates(file)\n",
    "\n",
    "        file = fileinput.FileInput(input_file, openhook=fileinput.hook_compressed)\n",
    "        for page_data in pages_from(file):\n",
    "            id, revid, title, ns, page = page_data\n",
    "            Extractor(id, revid, title, page).extract(sys.stdout)\n",
    "        file.close()\n",
    "        return\n",
    "\n",
    "    output_path = output\n",
    "    if output_path != '-' and not os.path.isdir(output_path):\n",
    "        try:\n",
    "            os.makedirs(output_path)\n",
    "        except:\n",
    "            logging.error('Could not create: %s', output_path)\n",
    "            return\n",
    "\n",
    "    process_dump(input_file, templates, output_path, file_size,\n",
    "                 compress, processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function builds actual dataset, as an input it takes the extracted dir and as output it will write to an npz file. It will walk over all the files in the extracted folder. Within the doc elements(it basically skips the lines with doc tags) it will tokenize each line by looking for `\\w<>` regelar expression. It will replace \".\" with special word <eos> end-of-sentece. The indexes of these tokens will then be added to the sequence, tokens that have not be found before will be added to the vocubalary to retrieve an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_dataset(extract_dir, target_path, max):\n",
    "    print max\n",
    "    seq = []\n",
    "    count = 0\n",
    "    words = {}# word => index, for fast index retrieval\n",
    "    word_list = []\n",
    "    last_index = 0\n",
    "    for current,dirs,files in os.walk(extract_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('wiki_'):\n",
    "                f = os.path.join(current, file)\n",
    "                with codecs.open(f,'r',encoding='utf8') as io:\n",
    "                    for line in io:\n",
    "                        # This regex matches <doc>  and </doc> tags in the generated files, which should be ignored\n",
    "                        if re.match(r\"\\<\\/?doc(.*)\\>\",line) is None:\n",
    "                            # removes <br> and replace . with <eos>\n",
    "                            line = line.replace(\"<br>\",\" \").replace(\".\",\" <eos>\")\n",
    "                            for token in re.findall(\"[\\w\\<\\>]+\",line):\n",
    "                                if token not in words:\n",
    "                                    words[token] = last_index\n",
    "                                    word_list.append(token)\n",
    "                                    last_index += 1\n",
    "                                count += 1\n",
    "                                seq.append(words[token])\n",
    "                        if count > max and max > -1:\n",
    "                            break\n",
    "            if count > max and max > -1:\n",
    "                break\n",
    "        if count > max and max > -1:\n",
    "            break\n",
    "\n",
    "    seq = np.array(seq, dtype=np.uint32)\n",
    "    words = np.array(word_list, dtype=np.dtype('str'))\n",
    "    with open(target_path,'w') as io:\n",
    "        np.savez(io, seq=seq, voc=words)\n",
    "    return seq, words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the chainer abstraction we will now link these two functions together to make a single function that given an dump url will: \n",
    "- Download that url and cache it\n",
    "- Extract it using the dump_extract function\n",
    "- build from the dumped a sequence array\n",
    "\n",
    "As per chainer convention, the resulting matrix will be cached and loaded when called again on the same machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_dataset(url, max=-1):\n",
    "    \"\"\"\n",
    "    Gets sequence dataset of wikipedia dump. Retrieved from here: https://dumps.wikimedia.org/backup-index.html\n",
    "    Will download the dataset, extract the content using WikiExtractor(https://github.com/attardi/wikiextractor-2) to extract content from dump\n",
    "    Next it will tokenize text and build a sequence array: a list of integers that represent word sequences and a vocabulary\n",
    "    where the index of each element will be the integer used in the sequence of that word in the seq array\n",
    "    Dataset is not shuffled and order of sentences will be respected on an article basis, \".\" will be replaces by <eos> tokens\n",
    "\n",
    "    :param: url link to the dump on wikipedia\n",
    "    :param: Limit number of tokens in sequence\n",
    "    :return: seq, voc\n",
    "    \"\"\"\n",
    "\n",
    "    def creator(path):\n",
    "        dump_path = download.cached_download(url)\n",
    "        tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "        # WikiExtractor needs .bz2 extension to function well\n",
    "        dump_sym = os.path.join(tmp_dir, 'dump.bz2')\n",
    "        os.symlink(dump_path, dump_sym)\n",
    "        print \"Extracting dump...\"\n",
    "\n",
    "        extract_dir = os.path.join(tmp_dir,'extracts')\n",
    "        extract_dump(dump_sym, extract_dir, quiet=True)\n",
    "\n",
    "        print \"Building vocabulary and sequence array..\"\n",
    "        seq,voc = _build_dataset(extract_dir, path,max)\n",
    "\n",
    "        # clean up temp file:\n",
    "        print \"Removing dump\"\n",
    "        shutil.rmtree(extract_dir)\n",
    "\n",
    "        return seq, voc\n",
    "\n",
    "    def loader(path):\n",
    "        with open(path) as io:\n",
    "            data = np.load(io)\n",
    "            return data['seq'],data['voc']\n",
    "\n",
    "\n",
    "\n",
    "    root = download.get_dataset_directory('svoss/chainer/wiki')\n",
    "    path = os.path.join(root, hashlib.md5(url).hexdigest()+(\"_%d\" % max)+\".npz\")\n",
    "    return download.cache_or_load_file(path, creator, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq,voc = get_wiki_dataset('https://dumps.wikimedia.org/nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
