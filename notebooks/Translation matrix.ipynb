{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from wiki_dataset import get_wiki_dataset\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import os\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.functions import softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation matrix\n",
    "In this notebook we use the two learned language models to learn an translation matrix. We do this by retraining a recurrent network. This model has to predict the probability distribution of the next word given the previous word embeddings of the what I call the fit language: in our case english. However the recurrent layer values will be obtained from the flow language: spanish. The embedding layer and recurrent layers will not be trained however, only a linear transformation layer between the embedding and recurrent layers. Also the linear layer from recurrent to network output has to be retrained. Since we cannot re-use it. The general idea however remains the same: to make sure that the flow patterns fits the one of the original embeddings that embedding has to be linearly transformed such that it fits in the flow of the flow language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This the network we will be training. The l0 layer will be the linear transformation layer training, the recurrent l1-l2 layer are the recurrent flow layers. Where as the embeded layer and the output predict will be from the fit model. This model is also defined in code/model.py. Note that we have to retrain the l3 layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TranslationMatrixRNN(chainer.Chain):\n",
    "    def __init__(self, n_units, n_vocab, train=True):\n",
    "        super(TranslationMatrixRNN, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l0=L.Linear(n_units, n_units, nobias=True),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_units = n_units\n",
    "\n",
    "        # Our linear transformation layer starts with\n",
    "        for param in self.l0.params():\n",
    "            param.data[...] = np.eye(n_units)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l0(h0)\n",
    "        h2 = self.l1(F.dropout(h1, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse results\n",
    "Once we trained the model we can examine the results. We will need the learned translation matrix model and the word embeddings of the original two language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English has vocubaly size of 15736\n",
      "Spanish has vocubaly size of 16509\n"
     ]
    }
   ],
   "source": [
    "from models import load_rnn_model\n",
    "from helpers import read_dataset\n",
    "def load_tmrnn_model(model_file,n_vocab,n_units):\n",
    "    \"\"\"\n",
    "    Loads pre-trained trasnaltion matrix rnn language model in a classifier object\n",
    "    Exact training procedure is explained in language model notebook\n",
    "    \"\"\"\n",
    "    model = L.Classifier(TranslationMatrixRNN(n_units, n_vocab ))\n",
    "    model.compute_accuracy = False\n",
    "    chainer.serializers.load_npz(model_file, model)\n",
    "    return model\n",
    "\n",
    "ENGLISH_FOLDER = \"../result-english/\"\n",
    "SPANISH_FOLDER = \"../result-spanish/\"\n",
    "TRANS_FOLDER = \"../result-trans/\"\n",
    "\n",
    "model_english = os.path.join(ENGLISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "model_spanish = os.path.join(SPANISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "model_trans = os.path.join(TRANS_FOLDER,'model_fit_en_flow_es_30')\n",
    "seq_en, voc_en = read_dataset(os.path.join(ENGLISH_FOLDER,'data.npz'))\n",
    "seq_es, voc_es = read_dataset(os.path.join(SPANISH_FOLDER,'data.npz'))\n",
    "print(\"English has vocubaly size of %d\" % len(voc_en))\n",
    "print(\"Spanish has vocubaly size of %d\" % len(voc_es))\n",
    "rnn_en = load_rnn_model(model_english,len(voc_en),650)\n",
    "rnn_es = load_rnn_model(model_spanish,len(voc_es),650)\n",
    "tmrnn = load_tmrnn_model(model_trans, len(voc_en),650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 401, 487, 45, 288]\n",
      "the\n",
      "1309 pueblo 1.15414505531\n",
      "11463 definidos 1.15229425961\n",
      "5774 proveniente 1.15197452175\n",
      "254 000 1.15127534051\n",
      "11634 ignorancia 1.14225533182\n",
      "2168 matemáticas 1.1381306797\n",
      "11831 argumenta 1.13665396969\n",
      "12374 vor 1.13277899351\n",
      "6129 checo 1.12781306275\n",
      "7996 escaleras 1.1267066339\n",
      "house\n",
      "1937 estrada 1.15229862184\n",
      "14516 válvula 1.14469017955\n",
      "10537 manufactura 1.14275755929\n",
      "5037 aceite 1.1361963191\n",
      "7540 proteínas 1.13199044231\n",
      "14265 ciudadela 1.12970709624\n",
      "6566 encabezada 1.12922698086\n",
      "15072 encuestas 1.12906708268\n",
      "3747 centrales 1.12775513898\n",
      "9550 tailandia 1.1268144965\n",
      "national\n",
      "159 impacto 1.1646993011\n",
      "4606 electrónica 1.16330005766\n",
      "8152 jugando 1.15874897734\n",
      "9852 gil 1.15713302497\n",
      "3400 php 1.13882280923\n",
      "7976 pau 1.13812396435\n",
      "14340 permanecen 1.13673320906\n",
      "10632 descendido 1.13261568912\n",
      "9038 ganados 1.13054859588\n",
      "6370 designación 1.1296080716\n",
      "a\n",
      "8153 service 1.14864263779\n",
      "915 sea 1.14637861033\n",
      "15562 daimler 1.14027998533\n",
      "1850 fundadores 1.13646963266\n",
      "4127 ganadores 1.13538145619\n",
      "6930 pirineos 1.13462931234\n",
      "1160 brazos 1.13261377646\n",
      "12024 hechizo 1.12843820514\n",
      "15177 cabaña 1.12809247969\n",
      "839 doce 1.12739188513\n",
      "mountain\n",
      "14077 escritorio 1.13141613093\n",
      "13744 descargas 1.13094278173\n",
      "6019 tani 1.12845745856\n",
      "14746 distritales 1.12807720111\n",
      "1948 casanova 1.12545767516\n",
      "3291 respecto 1.12271392146\n",
      "9430 define 1.12070188213\n",
      "7560 asociados 1.1202530243\n",
      "4608 interesó 1.11897370322\n",
      "15535 mediación 1.11849506447\n",
      "(5, 15736)\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance\n",
    "from operator import itemgetter\n",
    "from heapq import nlargest\n",
    "from models import load_rnn_model\n",
    "import re\n",
    "from chainer.functions.activation.softmax import softmax\n",
    "\n",
    "def tokenize(line):\n",
    "    line = line.replace(\"<br>\", \" \").replace(\". \", \" <eos> \").lower()\n",
    "    for token in re.findall(\"[\\w\\<\\>]+\", line):\n",
    "        yield token\n",
    "\n",
    "embed_en = rnn_en.predictor.embed.W.data\n",
    "embed_es = rnn_en.predictor.embed.W.data\n",
    "translation = tmrnn.predictor.l0.W.data\n",
    "translated_en = np.dot(embed_en, translation)\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, embedding_a, embedding_b, voc_a,voc_b):\n",
    "        self.em_a = embedding_a\n",
    "        self.em_b = embedding_b\n",
    "        self.voc_a = voc_a\n",
    "        self.voc_b = voc_b\n",
    "        self.inv_voc_a = self._create_inverse_voc(voc_a)\n",
    "        self.inv_voc_b = self._create_inverse_voc(voc_b)\n",
    "\n",
    "    def closest_to(self, words_a, top_n=10):\n",
    "        translations = []\n",
    "        idxs = [self.inv_voc_a[word_a] for word_a in words_a]\n",
    "        print(idxs)\n",
    "        embeddings = self.em_a[idxs,:]\n",
    "        \n",
    "        dist = scipy.spatial.distance.cdist(embeddings,self.em_b,'cosine')\n",
    "        \n",
    "        for x in range(dist.shape[0]):\n",
    "            print(words_a[x])\n",
    "            result = nlargest(top_n, enumerate(dist[x,:]), itemgetter(1))\n",
    "            for idx,r in result:\n",
    "                print(idx,self.voc_b[idx],r)\n",
    "        \n",
    "        return dist.shape\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _create_inverse_voc(self, voc):\n",
    "        \"\"\" Creates inverse vocabulary from word to index\n",
    "        \"\"\"\n",
    "        return dict([(word,idx) for idx,word in enumerate(voc)])\n",
    "    \n",
    "TOP_N = 5\n",
    "\n",
    "def create_inverse_voc(voc):\n",
    "    \"\"\" Creates inverse vocabulary from word to index\n",
    "    \"\"\"\n",
    "    return dict([(word,idx) for idx,word in enumerate(voc)])\n",
    "\n",
    "def map_line_to_seq(line,inverse_voc):\n",
    "    \"\"\" Converts a string(sentence) to a sequence of integers\n",
    "    Will also tokenize the sentence\n",
    "    \"\"\"\n",
    "    return [inverse_voc[w] if w in inverse_voc else inverse_voc['<below_th>'] for w in tokenize(line)]\n",
    "\n",
    "def fill_till_max(x,filler=-1,n=100):\n",
    "    \"\"\" Will make an array of fixed size n, will use x to fill this array. \n",
    "    If len(x) < n will fill the rest with filler. \n",
    "    \"\"\"\n",
    "    return [x[i] if len(x) > i else filler for i in range(n)]\n",
    "\n",
    "def map_seq_to_sentence(seq, voc):\n",
    "    \"\"\" Maps seqs back to a readable sentence\n",
    "    \"\"\"\n",
    "    return \" \".join([voc[int(w)] for w in seq]).replace(\" <eos>\",\".\")\n",
    "\n",
    "def generate_text(pred, voc, seeds=[],max_len=100):\n",
    "    pred.reset_state()\n",
    "    inverse_voc = create_inverse_voc(voc)\n",
    "    \n",
    "    # matrix of sentences in rows, words in columns\n",
    "    text_idx = np.array([fill_till_max(map_line_to_seq(s,inverse_voc),n=max_len) for s in seeds], dtype=np.int32)\n",
    "    \n",
    "    # i is the to predict word column\n",
    "    for i in range(2,max_len):\n",
    "        # Our input is all words before the one to predict\n",
    "        before = i-1\n",
    "        # create a batch looking max 35 words back\n",
    "        begin = max(0,before-35)\n",
    "        \n",
    "        #calculate probabilty\n",
    "        x = pred(text_idx[:,before])\n",
    "        d = softmax(x).data #convert to prob distribution\n",
    "        next_words = []\n",
    "        d = np.delete(d,0,1) #Ignore <below_th> keyword\n",
    "        for r in range(d.shape[0]):\n",
    "            top = nlargest(TOP_N, enumerate(d[r,:]), itemgetter(1))\n",
    "            idx = [x[0] for x in top]\n",
    "            probs = [x[1] for x in top]\n",
    "            probs = np.array(probs)/np.sum(probs) #normalize to valid prob. distribution\n",
    "            next_words.append(np.random.choice(idx,p=probs) + 1)\n",
    "    \n",
    "        for si, w in enumerate(next_words):\n",
    "            # only replace -1 values, those have to be generated\n",
    "            if text_idx[si,i] < 0:\n",
    "                text_idx[si,i] = w\n",
    "        \n",
    "    \n",
    "    return [map_seq_to_sentence(s, voc) for s in text_idx]\n",
    "\n",
    "#print(\"Generating for english:\")\n",
    "#seeds = [\"Sheep eat a lot of grass and produce wool. \", \"The Golden Rule or law of reciprocity is the principle of treating others as one would wish\"]\n",
    "#tmrnn.predictor.train = False\n",
    "#text = generate_text(tmrnn.predictor, voc_en, seeds)\n",
    "#for t in text:\n",
    "#    print(t)\n",
    "trans = Translator(translated_en, embed_es, voc_en, voc_es)\n",
    "print (trans.closest_to(['the','house','national','a','mountain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435.863\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
