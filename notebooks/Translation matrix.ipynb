{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from wiki_dataset import get_wiki_dataset\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import os\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.functions import softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation matrix\n",
    "In this notebook we use the two learned language models to learn an translation matrix. We do this by retraining a recurrent network. This model has to predict the probability distribution of the next word given the previous word embeddings of the what I call the fit language: in our case english. However the recurrent layer values will be obtained from the flow language: spanish. The embedding layer and recurrent layers will not be trained however, only a linear transformation layer between the embedding and recurrent layers. Also the linear layer from recurrent to network output has to be retrained. Since we cannot re-use it. The general idea however remains the same: to make sure that the flow patterns fits the one of the original embeddings that embedding has to be linearly transformed such that it fits in the flow of the flow language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This the network we will be training. The l0 layer will be the linear transformation layer training, the recurrent l1-l2 layer are the recurrent flow layers. Where as the embeded layer and the output predict will be from the fit model. This model is also defined in code/model.py. Note that we have to retrain the l3 layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TranslationMatrixRNN(chainer.Chain):\n",
    "    def __init__(self, n_units, n_vocab, train=True):\n",
    "        super(TranslationMatrixRNN, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l0=L.Linear(n_units, n_units, nobias=True),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_units = n_units\n",
    "\n",
    "        # Our linear transformation layer starts with\n",
    "        for param in self.l0.params():\n",
    "            param.data[...] = np.eye(n_units)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l0(h0)\n",
    "        h2 = self.l1(F.dropout(h1, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse results\n",
    "Once we trained the model we can examine the results. We will need the learned translation matrix model and the word embeddings of the original two language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English has vocubaly size of 15736\n",
      "Spanish has vocubaly size of 16509\n"
     ]
    }
   ],
   "source": [
    "from models import load_rnn_model\n",
    "from helpers import read_dataset\n",
    "def load_tmrnn_model(model_file,n_vocab,n_units):\n",
    "    \"\"\"\n",
    "    Loads pre-trained trasnaltion matrix rnn language model in a classifier object\n",
    "    Exact training procedure is explained in language model notebook\n",
    "    \"\"\"\n",
    "    model = L.Classifier(TranslationMatrixRNN(n_units, n_vocab ))\n",
    "    model.compute_accuracy = False\n",
    "    chainer.serializers.load_npz(model_file, model)\n",
    "    return model\n",
    "\n",
    "ENGLISH_FOLDER = \"../result-english/\"\n",
    "SPANISH_FOLDER = \"../result-spanish/\"\n",
    "TRANS_FOLDER = \"../result-trans/\"\n",
    "\n",
    "model_english = os.path.join(ENGLISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "model_spanish = os.path.join(SPANISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "model_trans = os.path.join(TRANS_FOLDER,'model_fit_en_flow_es_30')\n",
    "seq_en, voc_en = read_dataset(os.path.join(ENGLISH_FOLDER,'data.npz'))\n",
    "seq_es, voc_es = read_dataset(os.path.join(SPANISH_FOLDER,'data.npz'))\n",
    "print(\"English has vocubaly size of %d\" % len(voc_en))\n",
    "print(\"Spanish has vocubaly size of %d\" % len(voc_es))\n",
    "rnn_en = load_rnn_model(model_english,len(voc_en),650)\n",
    "rnn_es = load_rnn_model(model_spanish,len(voc_es),650)\n",
    "tmrnn = load_tmrnn_model(model_trans, len(voc_en),650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for english:\n",
      "(15736, 650) (16509, 650)\n",
      "\n",
      "the\n",
      "-----\n",
      "\n",
      "\n",
      "house\n",
      "-----\n",
      "\n",
      "\n",
      "national\n",
      "-----\n",
      "\n",
      "\n",
      "a\n",
      "-----\n",
      "\n",
      "\n",
      "mountain\n",
      "-----\n",
      "\n",
      "\n",
      "movie\n",
      "-----\n",
      "\n",
      "\n",
      "director\n",
      "-----\n",
      "\n",
      "\n",
      "one\n",
      "-----\n",
      "\n",
      "\\textbf{the} & \\textbf{house} & \\textbf{national} & \\textbf{a}\\\\ \\hline\n",
      "desarrollados: 0.1540 & nicola: 0.1575 & íntimos: 0.1560 & caracterizado: 0.1612\\\\\n",
      "mejor: 0.1520 & princess: 0.1387 & descubre: 0.1438 & carece: 0.1612\\\\\n",
      "apoyando: 0.1397 & fieles: 0.1326 & entiende: 0.1402 & recientemente: 0.1444\\\\\n",
      "agujeros: 0.1347 & músicos: 0.1289 & allen: 0.1339 & metacritic: 0.1421\\\\\n",
      "encuesta: 0.1341 & occidental: 0.1285 & clay: 0.1324 & aghion: 0.1356\\\\\n",
      "nos: 0.1305 & escorial: 0.1269 & buemi: 0.1321 & conformada: 0.1330\\\\\n",
      "rurales: 0.1302 & convertiría: 0.1267 & ong: 0.1288 & gradas: 0.1320\\\\\n",
      "hande: 0.1268 & camille: 0.1233 & tauromaquia: 0.1285 & distribución: 0.1295\\\\\n",
      "siguientes: 0.1259 & anualmente: 0.1231 & tampoco: 0.1284 & quintana: 0.1259\\\\\n",
      "sucedida: 0.1254 & thomas: 0.1229 & malo: 0.1278 & secretariado: 0.1251\\\\\n",
      "\\textbf{mountain} & \\textbf{movie} & \\textbf{director} & \\textbf{one}\\\\ \\hline\n",
      "modernización: 0.1454 & 1859: 0.1576 & azul: 0.1581 & claude: 0.1637\\\\\n",
      "solamente: 0.1286 & darmstadt: 0.1530 & bio: 0.1469 & benoît: 0.1556\\\\\n",
      "disputándose: 0.1271 & cercomacroides: 0.1511 & fluvial: 0.1446 & adopta: 0.1466\\\\\n",
      "todo: 0.1264 & complete: 0.1464 & completaron: 0.1434 & clip: 0.1441\\\\\n",
      "premiado: 0.1260 & ryan: 0.1464 & ix: 0.1432 & betulae: 0.1425\\\\\n",
      "dibujo: 0.1259 & sencilla: 0.1397 & calabaza: 0.1401 & faceta: 0.1420\\\\\n",
      "traslado: 0.1249 & caate: 0.1366 & salieron: 0.1347 & donde: 0.1405\\\\\n",
      "2016<: 0.1246 & pedro: 0.1357 & lírico: 0.1336 & iban: 0.1378\\\\\n",
      "relevos: 0.1242 & profesional: 0.1345 & participaciones: 0.1332 & otro: 0.1368\\\\\n",
      "irene: 0.1234 & clan: 0.1326 & gobierno: 0.1318 & murallas: 0.1361\\\\\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance\n",
    "from operator import itemgetter\n",
    "from heapq import nlargest\n",
    "from models import load_rnn_model\n",
    "import re\n",
    "from chainer.functions.activation.softmax import softmax\n",
    "\n",
    "def tokenize(line):\n",
    "    line = line.replace(\"<br>\", \" \").replace(\". \", \" <eos> \").lower()\n",
    "    for token in re.findall(\"[\\w\\<\\>]+\", line):\n",
    "        yield token\n",
    "        \n",
    "def _normalize(word_vec):\n",
    "    norm=np.linalg.norm(word_vec)\n",
    "    if norm == 0: \n",
    "        return word_vec\n",
    "    return word_vec/norm\n",
    "\n",
    "embed_en = rnn_en.predictor.embed.W.data\n",
    "embed_es = rnn_es.predictor.embed.W.data\n",
    "translation = tmrnn.predictor.l0.W.data\n",
    "translated_en = np.dot(embed_en, translation)\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, embedding_a, embedding_b, voc_a,voc_b):\n",
    "        self.em_a = embedding_a\n",
    "        self.em_b = embedding_b\n",
    "        self.voc_a = voc_a\n",
    "        self.voc_b = voc_b\n",
    "        self.inv_voc_a = self._create_inverse_voc(voc_a)\n",
    "        self.inv_voc_b = self._create_inverse_voc(voc_b)\n",
    "    \n",
    "    def do_norm(self):\n",
    "        N = []\n",
    "        for x in range(self.em_a.shape[0]):\n",
    "            vec = _normalize(self.em_a[x,:])\n",
    "            N.append(vec)\n",
    "        self.em_a = np.array(N)\n",
    "\n",
    "        N = []\n",
    "        for x in range(self.em_b.shape[0]):\n",
    "            vec = _normalize(self.em_b[x,:])\n",
    "            N.append(vec) \n",
    "            \n",
    "        self.em_b = np.array(N)\n",
    "        \n",
    "\n",
    "    def closest_to(self, words_a, top_n=10):\n",
    "        translations = []\n",
    "        idxs = [self.inv_voc_a[word_a] for word_a in words_a]\n",
    "     \n",
    "        embeddings = self.em_a[idxs,:]\n",
    "        \n",
    "        dist = 1. - scipy.spatial.distance.cdist(embeddings,self.em_b,'cosine')\n",
    "        \n",
    "        for x in range(dist.shape[0]):\n",
    "            print(\"\\n%s\\n-----\\n\" % words_a[x])\n",
    "            result = nlargest(top_n, enumerate(dist[x,:]), itemgetter(1))\n",
    "            for idx,r in result:\n",
    "                print(idx,self.voc_b[idx],r)\n",
    "        \n",
    "        return dist.shape\n",
    "    \n",
    "    def to_latex_table(self, words_a, top_n=10,per=4):\n",
    "        translations = []\n",
    "        idxs = [self.inv_voc_a[word_a] for word_a in words_a]\n",
    "     \n",
    "        embeddings = self.em_a[idxs,:]\n",
    "        \n",
    "        dist = 1. - scipy.spatial.distance.cdist(embeddings,self.em_b,'cosine')\n",
    "        per_row = [[] for x in range(top_n)]\n",
    "        for x in range(dist.shape[0]):\n",
    "            print(\"\\n%s\\n-----\\n\" % words_a[x])\n",
    "            result = nlargest(top_n, enumerate(dist[x,:]), itemgetter(1))\n",
    "            row = 0\n",
    "            for idx,r in result:\n",
    "                per_row[row].append((idx,r))\n",
    "                row += 1\n",
    "        x = 0\n",
    "        while x < len(words_a):\n",
    "            end = min(len(words_a), x + 4)\n",
    "            self.print_table(words_a[x:end], [row[x:end] for row in per_row])\n",
    "            x =end\n",
    "    \n",
    "    def print_table(self,names, rows):\n",
    "        \n",
    "        print(\" & \".join([\"\\\\textbf{%s}\" % a for a in names ]) + \"\\\\\\ \\hline\")\n",
    "        for row in rows:\n",
    "            print(\" & \".join([\"%s: %.4f\" % (self.voc_b[c[0]], c[1]) for c in row]) + \"\\\\\\\\\")\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def _create_inverse_voc(self, voc):\n",
    "        \"\"\" Creates inverse vocabulary from word to index\n",
    "        \"\"\"\n",
    "        return dict([(word,idx) for idx,word in enumerate(voc)])\n",
    "    \n",
    "TOP_N = 5\n",
    "\n",
    "def create_inverse_voc(voc):\n",
    "    \"\"\" Creates inverse vocabulary from word to index\n",
    "    \"\"\"\n",
    "    return dict([(word,idx) for idx,word in enumerate(voc)])\n",
    "\n",
    "def map_line_to_seq(line,inverse_voc):\n",
    "    \"\"\" Converts a string(sentence) to a sequence of integers\n",
    "    Will also tokenize the sentence\n",
    "    \"\"\"\n",
    "    return [inverse_voc[w] if w in inverse_voc else inverse_voc['<below_th>'] for w in tokenize(line)]\n",
    "\n",
    "def fill_till_max(x,filler=-1,n=100):\n",
    "    \"\"\" Will make an array of fixed size n, will use x to fill this array. \n",
    "    If len(x) < n will fill the rest with filler. \n",
    "    \"\"\"\n",
    "    return [x[i] if len(x) > i else filler for i in range(n)]\n",
    "\n",
    "def map_seq_to_sentence(seq, voc):\n",
    "    \"\"\" Maps seqs back to a readable sentence\n",
    "    \"\"\"\n",
    "    return \" \".join([voc[int(w)] for w in seq]).replace(\" <eos>\",\".\")\n",
    "\n",
    "def generate_text(pred, voc, seeds=[],max_len=100):\n",
    "    pred.reset_state()\n",
    "    inverse_voc = create_inverse_voc(voc)\n",
    "    \n",
    "    # matrix of sentences in rows, words in columns\n",
    "    text_idx = np.array([fill_till_max(map_line_to_seq(s,inverse_voc),n=max_len) for s in seeds], dtype=np.int32)\n",
    "    \n",
    "    # i is the to predict word column\n",
    "    for i in range(2,max_len):\n",
    "        # Our input is all words before the one to predict\n",
    "        before = i-1\n",
    "        # create a batch looking max 35 words back\n",
    "        begin = max(0,before-35)\n",
    "        \n",
    "        #calculate probabilty\n",
    "        x = pred(text_idx[:,before])\n",
    "        d = softmax(x).data #convert to prob distribution\n",
    "        next_words = []\n",
    "        d = np.delete(d,0,1) #Ignore <below_th> keyword\n",
    "        for r in range(d.shape[0]):\n",
    "            top = nlargest(TOP_N, enumerate(d[r,:]), itemgetter(1))\n",
    "            idx = [x[0] for x in top]\n",
    "            probs = [x[1] for x in top]\n",
    "            probs = np.array(probs)/np.sum(probs) #normalize to valid prob. distribution\n",
    "            next_words.append(np.random.choice(idx,p=probs) + 1)\n",
    "    \n",
    "        for si, w in enumerate(next_words):\n",
    "            # only replace -1 values, those have to be generated\n",
    "            if text_idx[si,i] < 0:\n",
    "                text_idx[si,i] = w\n",
    "        \n",
    "    \n",
    "    return [map_seq_to_sentence(s, voc) for s in text_idx]\n",
    "\n",
    "print(\"Generating for english:\")\n",
    "#seeds = [\"Sheep eat a lot of grass and produce wool. \", \"The Golden Rule or law of reciprocity is the principle of treating others as one would wish\"]\n",
    "#tmrnn.predictor.train = False\n",
    "#text = generate_text(tmrnn.predictor, voc_en, seeds)\n",
    "#for t in text:\n",
    "#    print(t)\n",
    "print(translated_en.shape, embed_es.shape)\n",
    "trans = Translator(translated_en, embed_es, voc_en, voc_es)\n",
    "trans.do_norm()\n",
    "print (trans.to_latex_table(['the','house','national','a','mountain','movie','director','one']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435.863\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
