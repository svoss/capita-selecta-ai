{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from collections import Counter, OrderedDict\n",
    "path = \"/Users/stijnvoss/Documents/uni/capita-selecta-ai/datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an word2vec model we have to split our text in sentences and words. It's not trivial for the voynich manuscript tough: https://stephenbax.net/?p=940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctation(sentence):\n",
    "    \"\"\"\n",
    "    Given an sentence represented as a list of tokens, remove the tokens from the list that do not contain \"\"\n",
    "    \"\"\"\n",
    "    return [t.lower() for t in sentence if re.search(\"\\w\",t) is not None]\n",
    "\n",
    "def prepare_sentences(folder):\n",
    "    #walk over all files in folder\n",
    "    s = 0\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with codecs.open(os.path.join(folder,filename),'r',encoding='utf8') as io:\n",
    "                for s in sent_tokenize(io.read()):\n",
    "                    yield word_tokenize(s)\n",
    "\n",
    "def plot_statistics(folder,index):\n",
    "    counter = OrderedDict(sorted([(int(k), int(v)) for k,v in Counter([len(s) for s in prepare_sentences(folder)]).iteritems()]))\n",
    "    print counter\n",
    "    #sentence\n",
    "    plt.figure(index)\n",
    "    plt.subplot(211)\n",
    "    plt.title(\"Number of words per sentence\")\n",
    "    plt.plot(counter.keys(), counter.values())\n",
    "    \n",
    "    tokens = [t for s in prepare_sentences(folder) for t in s ]\n",
    "    token_counter =  OrderedDict(Counter(tokens))\n",
    "    plt.subplot(212)\n",
    "    plt.title(\"Word frequency\")\n",
    "    plt.plot(sorted(token_counter.values(),reverse=True))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own dataset (with similar properties as the voynisch manuscript)\n",
    "First let's try to investigate how word2vec could be used on the voynish manuscript by using our . I had to finetune the word2vec params a bit. Partly trough doing some best practice research, partly by doing some experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES\n",
      "[(u'que', 0.9946640729904175), (u'a', 0.994613766670227), (u'se', 0.9935480356216431), (u'y', 0.9924092888832092), (u'el', 0.9922371506690979), (u'de', 0.9912117719650269), (u'en', 0.9903433322906494), (u'los', 0.9897137880325317), (u'como', 0.9878660440444946), (u'con', 0.9876834154129028)]\n",
      "[(u'una', 0.8490885496139526), (u'arroz', 0.8478215336799622), (u'petroselinum', 0.8423986434936523), (u'm\\xe1s', 0.8422155380249023), (u'sus', 0.8415376543998718), (u'tambi\\xe9n', 0.8353066444396973), (u'son', 0.8339482545852661), (u'lo', 0.8319666981697083), (u'sistema', 0.8316299915313721), (u'planta', 0.822829008102417)]\n",
      "0.818549321368 0.678316980796 0.942374482885\n",
      "---\n",
      "NL\n",
      "[(u'van', 0.9911580085754395), (u'het', 0.9809150695800781), (u'een', 0.9787498712539673), (u'in', 0.977186381816864), (u'worden', 0.9732418656349182), (u'en', 0.9711448550224304), (u'is', 0.9703822135925293), (u'op', 0.964158296585083), (u'voor', 0.9587825536727905), (u'kan', 0.9569529891014099)]\n",
      "[(u'zaaien', 0.654285192489624), (u'new', 0.6313918828964233), (u'schijnbaar', 0.5883742570877075), (u'toegedicht', 0.5810410380363464), (u'vinca', 0.5732792019844055), (u'uitsluitend', 0.5507131814956665), (u'tegengaan', 0.5447786450386047), (u'gemalen', 0.5295045971870422), (u'wijnruit', 0.5275577902793884), (u'oorspronkelijke', 0.5271339416503906)]\n",
      "-0.643685183917 -0.32520439581 0.914488536915\n",
      "---\n",
      "EN\n",
      "[(u'as', 0.9949923157691956), (u'of', 0.9949795007705688), (u'is', 0.993736743927002), (u'and', 0.9922729730606079), (u'a', 0.9920644760131836), (u'in', 0.9912185072898865), (u'with', 0.9911590218544006), (u'from', 0.9901672601699829), (u'including', 0.9897738099098206), (u'at', 0.9897361397743225)]\n",
      "[(u'the', 0.9765239953994751), (u'new', 0.973198652267456), (u'of', 0.9720253944396973), (u'on', 0.9712490439414978), (u'it', 0.9707912802696228), (u'that', 0.9694281816482544), (u'drug', 0.9690628051757812), (u\"'s\", 0.9672317504882812), (u'including', 0.967088520526886), (u'at', 0.9670826196670532)]\n",
      "0.696884260863 0.527579758357 0.980586702787\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "sentences = [remove_punctation(s) for s in prepare_sentences(os.path.join(path,'es'))]\n",
    "es = word2vec.Word2Vec(sentences, size=20, window=5, min_count=2, workers=4,sg=1,hs=1, alpha=0.00025,iter=50)\n",
    "print \"ES\"\n",
    "print es.most_similar('la')\n",
    "print es.most_similar('menta')\n",
    "print es.similarity('menta','el'),es.similarity('menta','romero'),es.similarity('dios','la')\n",
    "print \"---\"\n",
    "\n",
    "sentences = [remove_punctation(s) for s in prepare_sentences(os.path.join(path,'nl'))]\n",
    "nl = word2vec.Word2Vec(sentences, size=20, window=5, min_count=2, workers=4,sg=1,hs=1, alpha=0.00025,iter=50)\n",
    "print \"NL\"\n",
    "print nl.most_similar('de')\n",
    "print nl.most_similar('mint')\n",
    "print nl.similarity('mint','hem'),nl.similarity('mint','anijs'),nl.similarity('god','de')\n",
    "print \"---\"\n",
    "\n",
    "sentences = [remove_punctation(s) for s in prepare_sentences(os.path.join(path,'en'))]\n",
    "en = word2vec.Word2Vec(sentences, size=20, window=5, min_count=2, workers=4,sg=1,hs=1, alpha=0.00025,iter=50)\n",
    "print \"EN\"\n",
    "print en.most_similar('the')\n",
    "print en.most_similar('mint')\n",
    "print en.similarity('mint','him'),en.similarity('mint','rosemary'),en.similarity('god','the')\n",
    "print \"---\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec models perform far from perfect as could be expected from the amount of data we have. But still we see some useful patterns. We see that articles are pretty close together in the vector space, we also see some herbs/plants grouped together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also supports learning word2vec directly from wikipedia, which may lead to more usable word2vec values. Because of the way larger vocubaries at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es = os.path.join(path,'es')\n",
    "wiki = WikiCorpus(os.path.join(es,'eswiki-20161120-pages-articles1.xml-p000000005p000229076.bz2'))\n",
    "MmCorpus.serialize(os.path.join(es,'wiki.mm'), wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
