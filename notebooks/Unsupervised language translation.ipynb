{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from wiki_dataset import get_wiki_dataset\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "We want to see if we can use the general sequential patterns in textual data to learn word 2 word alignments in a unsupervised manner. Given two datasets for two languages we will first learn a language model for each language: a recurrent neural network that tries to predict the next word given the sequence of words before.\n",
    "\n",
    "For each model we will try to predict the values of the next word embedding value. In the next phase we will use the language model of language a to predict the words of language b. Hence the input embeddings and output embeddings will be of language b where as all other parameters will be of language a. In this phase a transformation matrix will be added that is the only that will be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Our model is based on the https://github.com/pfnet/chainer/tree/master/examples/ptb model. However we add an extra weights layer that can later be used to linearly transform the input matrix. In the beginning it will be an identity matrix and parameters will not be updated. In the next phase it will be the only one that is learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This a simple language model, directly copied from the tutorial\n",
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_units = n_units\n",
    "        self.n_vocab = n_vocab\n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        #for param in self.params():\n",
    "        #    param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h2 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
    "        return y\n",
    "\n",
    "# This network builds a bit different network that add layers that we need to be able to learn our translation\n",
    "# First we need the translation layer \n",
    "# Secondly we need to the predict the next embedding instead of the word itself. If not we cannot re-use the network for the other language\n",
    "# We can however also no train the word embedding immid\n",
    "class TransformingRNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, lm, train=True):\n",
    "        super(TransformingRNNLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l0=L.Linear(n_units,n_units,nobias=True),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        \n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        \n",
    "        # Our linear tranformation layer starts with \n",
    "        for param in self.l0.params():\n",
    "            param.data[...] = np.eye(n_units)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l0(h0)\n",
    "        h2 = self.l1(F.dropout(h2, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h3, train=self.train))\n",
    "        y = self.l3(F.dropout(h2, train=self.train))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNNForLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e5dfe2327d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNForLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m86000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RNNForLM' is not defined"
     ]
    }
   ],
   "source": [
    "x = RNNForLM(86000,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset iterator to create a batch of sequences at different positions.\n",
    "# This iterator returns a pair of current words and the next words. Each\n",
    "# example is a part of sequences starting from the different offsets\n",
    "# equally spaced within the whole sequence.\n",
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of ``__next__``.\n",
    "        self.iteration = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "\n",
    "\n",
    "# Custom updater for truncated BackProp Through Time (BPTT)\n",
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters\n",
    "\n",
    "\n",
    "# Routine to rewrite the result dictionary of LogReport to add perplexity\n",
    "# values\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "First we have to divide our training, test and validation set. I use roughlt the same fractionas are used in the get_ptb_words() dataset. First 10% - 90% training,validation. Then split the training and validation in 10-90% again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-04a931cdb9d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrain_phase1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dutch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-04a931cdb9d1>\u001b[0m in \u001b[0;36mtrain_phase1\u001b[0;34m(dump, name, test_mode, epoch, batch_size, gpu, out, grad_clip, brpoplen, resume)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mn_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#n_vocab=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-04a931cdb9d1>\u001b[0m in \u001b[0;36mretrieve_and_split\u001b[0;34m(dump)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wiki_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stijnvoss/Documents/UNI/capita-selecta-ai/code/wiki_dataset.pyc\u001b[0m in \u001b[0;36mget_wiki_dataset\u001b[0;34m(url, max, th)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svoss/chainer/wiki'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_%d-%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_or_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/chainer/dataset/download.pyc\u001b[0m in \u001b[0;36mcache_or_load_file\u001b[0;34m(path, creator, loader)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stijnvoss/Documents/UNI/capita-selecta-ai/code/wiki_dataset.pyc\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdump_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/chainer/dataset/download.pyc\u001b[0m in \u001b[0;36mcached_download\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading from {}...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_local_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_local_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen_local_file\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mmodified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musegmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2'"
     ]
    }
   ],
   "source": [
    "def retrieve_and_split(dump):\n",
    "    seq, voc = get_wiki_dataset(dump)\n",
    "    seq = seq.astype(np.int32)\n",
    "    \n",
    "    val_start = int(len(seq) * .9)\n",
    "    test_start = int(val_start *.9)\n",
    "    train = seq[:test_start]\n",
    "    test = seq[test_start:val_start]\n",
    "    val = seq[val_start:]\n",
    "    \n",
    "    return train, val, test, voc\n",
    "\n",
    "def train_phase1(dump, name, test_mode=False, epoch=5, batch_size=128, gpu=-1,out='result', grad_clip=True, brpoplen=35,resume=''):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    train, val, test,voc = retrieve_and_split(dump)\n",
    "    n_vocab = len(voc)\n",
    "    #n_vocab=10\n",
    "    print(\"Going to run %s\" % name) \n",
    "    print(\"#training: %d, #val: %d, #test: %d\" %(len(train), len(val), len(test)))\n",
    "    print(\"#vocabulary: %d\" % n_vocab)\n",
    "    \n",
    "    if test_mode:\n",
    "        print(\"Running in test mode, cutting test,train and val set to 100 elements each\")\n",
    "        train = train[:100]\n",
    "        test = test[:100]\n",
    "        val = val[:100]\n",
    "   \n",
    "    train_iter = ParallelSequentialIterator(train, batch_size)\n",
    "    val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "    test_iter = ParallelSequentialIterator(test, 1, repeat=False)\n",
    "\n",
    "    # Prepare an RNNLM model\n",
    "    print(\"Creating model\")\n",
    "    rnn = RNNLM(n_vocab, 800)\n",
    "    print(\"Init model complete\")\n",
    "    model = L.Classifier(rnn)\n",
    "    model.compute_accuracy = False  # we only want the perplexity\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # make the GPU current\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Set up an optimizer\n",
    "    optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.GradientClipping(grad_clip))\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = BPTTUpdater(train_iter, optimizer, brpoplen, gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    eval_model = model.copy()  # Model with shared params and distinct states\n",
    "    eval_rnn = eval_model.predictor\n",
    "    eval_rnn.train = False\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "        val_iter, eval_model, device=gpu,\n",
    "        # Reset the RNN state at the beginning of each evaluation\n",
    "        eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "    interval = 10 if test_mode else 500\n",
    "    trainer.extend(extensions.LogReport(postprocess=compute_perplexity,\n",
    "                                        trigger=(interval, 'iteration')))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'iteration', 'perplexity', 'val_perplexity']\n",
    "    ), trigger=(interval, 'iteration'))\n",
    "    trainer.extend(extensions.ProgressBar(\n",
    "        update_interval=1 if test_mode else 10))\n",
    "    trainer.extend(extensions.snapshot())\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'model_iter_{.updater.iteration}'))\n",
    "    if resume:\n",
    "        chainer.serializers.load_npz(resume, trainer)\n",
    "\n",
    "    trainer.run()\n",
    "\n",
    "    # Evaluate the final model\n",
    "    print('test')\n",
    "    eval_rnn.reset_state()\n",
    "    evaluator = extensions.Evaluator(test_iter, eval_model, device=gpu)\n",
    "    result = evaluator()\n",
    "    print('test perplexity:', np.exp(float(result['main/loss'])))\n",
    "    \n",
    "    \n",
    "train_phase1('nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2', 'dutch', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
