{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from wiki_dataset import get_wiki_dataset\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import os\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.functions import softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "In this notebook I will train the language models that I can use in a later stadium to try to learn the linear mapping from model a to model b. In this model we get as input a word embedding and as output the next word in the sequence. \n",
    "\n",
    "I will train a model for spanish and english."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Our model is based on the https://github.com/pfnet/chainer/tree/master/examples/ptb model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This a simple language model, directly copied from the tutorial\n",
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_units = n_units\n",
    "        self.n_vocab = n_vocab\n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        #for param in self.params():\n",
    "        #    param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h2 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tutorial we re-use the following code:\n",
    "- **ParallelSequentialIteratior**: This class takes a dataset and creates batches(with batch size N) from it. In the first \\_\\_next()\\_\\_ step it will give the first words of N sequences as input and as target value the next word in the sequence. In the next \\_\\_next\\_\\_() call it will return the 2nd word of N sequences as input and the third word for as target etc. Running multiple sequences at the same time allows for efficient matrix multiplication(especially useful for GPU acceleration) furthermore the gradients for all sequences can be summed and applied immediately(allowing for statistic gradient descent). \n",
    "- **BPTTUpdater**: To obtain the weight gradients for our neural network we make use of back propagation trough time algorithm. It unfolds a neural network into a normal feedforward network where each layer represents one timestep, and then sums the gradients for each connection. The gradients are only saved for a fixed number of steps (\"truncated\"). \n",
    "- **Compute perplexity**: Perplexity is a measurement often used to evaluate language models in NLP context. It  assumes to obtain the softmax classification error as is used by default by the [chainer L.Classifier link](http://docs.chainer.org/en/stable/_modules/chainer/links/model/classifier.html?highlight=Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset iterator to create a batch of sequences at different positions.\n",
    "# This iterator returns a pair of current words and the next words. Each\n",
    "# example is a part of sequences starting from the different offsets\n",
    "# equally spaced within the whole sequence.\n",
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of ``__next__``.\n",
    "        self.iteration = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "\n",
    "\n",
    "# Custom updater for truncated BackProp Through Time (BPTT)\n",
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters\n",
    "\n",
    "\n",
    "# Routine to rewrite the result dictionary of LogReport to add perplexity\n",
    "# values\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "First we have to divide our training, test and validation set. I use rougly the same fractions that are used in the get_ptb_words() dataset. First a 10% - 90% training - validation split. Then split the training again 10% - 90% to obtain a test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_and_split(dump):\n",
    "    seq, voc = get_wiki_dataset(dump)\n",
    "    seq = seq.astype(np.int32)\n",
    "    \n",
    "    val_start = int(len(seq) * .9)\n",
    "    test_start = int(val_start *.9)\n",
    "    train = seq[:test_start]\n",
    "    test = seq[test_start:val_start]\n",
    "    val = seq[val_start:]\n",
    "    \n",
    "    return train, val, test, voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dump, name, test_mode=False, epoch=5, batch_size=128, gpu=-1, out='result', grad_clip=True, brpoplen=35, resume='',max_seq_size=250000):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    train, val, test, voc = retrieve_and_split(dump,max_seq_size)\n",
    "    n_vocab = len(voc)\n",
    "    # n_vocab=10\n",
    "    print(\"Going to run %s\" % name)\n",
    "    print(\"#training: %d, #val: %d, #test: %d\" % (len(train), len(val), len(test)))\n",
    "    print(\"#vocabulary: %d\" % n_vocab)\n",
    "\n",
    "    if test_mode:\n",
    "        print(\"Running in test mode: cutting test, train and val set to 100 elements each\")\n",
    "        train = train[:10000]\n",
    "        test = test[:100]\n",
    "        val = val[:100]\n",
    "        if batch_size > 100:\n",
    "            batch_size = 100\n",
    "\n",
    "    train_iter = ParallelSequentialIterator(train, batch_size)\n",
    "    val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "    test_iter = ParallelSequentialIterator(test, 1, repeat=False)\n",
    "\n",
    "    # Prepare an RNNLM model\n",
    "    print(\"Creating model\")\n",
    "    rnn = RNNLM(n_vocab, 800)\n",
    "    print(\"Init model complete\")\n",
    "    model = L.Classifier(rnn)\n",
    "    model.compute_accuracy = False  # we only want the perplexity\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # make the GPU current\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Set up an optimizer\n",
    "    optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.GradientClipping(grad_clip))\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = BPTTUpdater(train_iter, optimizer, brpoplen, gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    eval_model = model.copy()  # Model with shared params and distinct states\n",
    "    eval_rnn = eval_model.predictor\n",
    "    eval_rnn.train = False\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "        val_iter, eval_model, device=gpu,\n",
    "        # Reset the RNN state at the beginning of each evaluation\n",
    "        eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "    interval = 10 if test_mode else 500\n",
    "\n",
    "    trainer.extend(extensions.ExponentialShift('lr', 0.5),\n",
    "                   trigger=(25, 'epoch'))\n",
    "    trainer.extend(extensions.LogReport(postprocess=compute_perplexity,\n",
    "                                        trigger=(interval, 'iteration')))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'iteration', 'perplexity', 'val_perplexity']\n",
    "    ), trigger=(interval, 'iteration'))\n",
    "    trainer.extend(extensions.ProgressBar(\n",
    "        update_interval=10 if test_mode else 125))\n",
    "    \n",
    "    #this extension makes sure our parameters are saved to the disk so we can use it later\n",
    "    trainer.extend(extensions.snapshot())\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'model_iter_{.updater.iteration}'))\n",
    "    if resume:\n",
    "        chainer.serializers.load_npz(resume, trainer)\n",
    "    date = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    fn_a = 'loss_%s.png' % date\n",
    "    loss_r = extensions.PlotReport(['validation/main/loss','main/loss'],'epoch',file_name=fn_a)\n",
    "\n",
    "    trainer.extend(loss_r)\n",
    "    start = time.time()\n",
    "    trainer.run()\n",
    "\n",
    "    diff = time.time() - start\n",
    "    \n",
    "    # Evaluate the final model\n",
    "    print('test')\n",
    "    eval_rnn.reset_state()\n",
    "    evaluator = extensions.Evaluator(test_iter, eval_model, device=gpu)\n",
    "    result = evaluator()\n",
    "    print('test perplexity:', np.exp(float(result['main/loss'])))\n",
    "    com.add_text(\"final loss\",result['main/loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I actually executed this script via a batch script for english and spanish on a GPU machine. Using the `sh scripts/english_lm.sh` and `sh scripts/spanish_lm.sh`. These bash scripts use the `code/lm.py` script to perform the training. Which will import shared code from helpers.py and models.py. I copy the generated plots, latest snapshot and exact dataset(using `sh scripts/dump_(english|spanish).sh` to my local machine.\n",
    "\n",
    "## Analyse learned language models\n",
    "We will now see if the learned language models are well trained language models by letting the network generate some text and by mapping the embedding to a 2d space and see if we can find any linear similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models import load_rnn_model\n",
    "import re\n",
    "from chainer.functions.activation.softmax import softmax\n",
    "def tokenize(line):\n",
    "    line = line.replace(\"<br>\", \" \").replace(\". \", \" <eos> \").lower()\n",
    "    for token in re.findall(\"[\\w\\<\\>]+\", line):\n",
    "        yield token\n",
    "def read_dataset(file):\n",
    "    \"\"\"\n",
    "    Reads dataset that was exported by export_dataset\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.load(file)\n",
    "    data = np.load(file)\n",
    "    seq,voc = data['seq'],data['voc']\n",
    "    seq = seq.astype(np.int32)\n",
    "    return seq,voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets and models\n",
    "This loads the datasets and parameter matrixes for the network into memory. These files are obtained by manually running `sh scripts/spanish_lm.sh` and `sh scripts/spanish_en.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/stijnvoss/Documents/UNI/home/unsupervised-language-translation-using-rnn/notebooks/../result-english/data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f16a30993ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_english\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENGLISH_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'650_u_45_e_1M_v_5_th_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_spanish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPANISH_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'650_u_45_e_1M_v_5_th_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mseq_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENGLISH_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mseq_es\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_es\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPANISH_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrnn_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_rnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_english\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m650\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7da5e5282915>\u001b[0m in \u001b[0;36mread_dataset\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'voc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/stijnvoss/Documents/UNI/home/unsupervised-language-translation-using-rnn/notebooks/../result-english/data.npz'"
     ]
    }
   ],
   "source": [
    "#refer to data obtained from our machine\n",
    "ENGLISH_FOLDER = os.path.join(os.getcwd(),\"../result-english/\")\n",
    "SPANISH_FOLDER = os.path.join(os.getcwd(),\"../result-spanish/\")\n",
    "model_english = os.path.join(ENGLISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "model_spanish = os.path.join(SPANISH_FOLDER,'650_u_45_e_1M_v_5_th_model')\n",
    "seq_en, voc_en = read_dataset(os.path.join(ENGLISH_FOLDER,'data.npz'))\n",
    "seq_es, voc_es = read_dataset(os.path.join(SPANISH_FOLDER,'data.npz'))\n",
    "rnn_en = load_rnn_model(model_english,len(voc_en),650)\n",
    "rnn_es = load_rnn_model(model_spanish,len(voc_es),650)\n",
    "print(\"The english vocabulary contains %d different words\" % len(voc_en))\n",
    "print(\"The spanish vocabulary contains %d different words\" % len(voc_es))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating text\n",
    "We will now generate some text from both the models. We first feed our network with a seed. After this we predict the next word of the network, we normalize the top-5 highest probablities and scale these 5 probablities linearly to a valid probablity distribution, randomly pick one, and seed that one. We repeat this process untill we reach a text of 100 words. We always ignore the <below_th> catch-all word. We found that if we didn't sample the words, but just pick the highest probability, the network would be stuck in a more repitative pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for english:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rnn_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a071dd96e400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating for english:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sheep eat a lot of grass and produce wool. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The Golden Rule or law of reciprocity is the principle of treating others as one would wish\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mrnn_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn_en' is not defined"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from heapq import nlargest\n",
    "TOP_N = 5 # Number of \n",
    "\n",
    "# Shared helping functions\n",
    "def create_inverse_voc(voc):\n",
    "    \"\"\" Creates inverse vocabulary from word to index\n",
    "    \"\"\"\n",
    "    return dict([(word,idx) for idx,word in enumerate(voc)])\n",
    "\n",
    "\n",
    "def map_line_to_seq(line,inverse_voc):\n",
    "    \"\"\" Converts a string(sentence) to a sequence of integers\n",
    "    Will also tokenize the sentence\n",
    "    \"\"\"\n",
    "    return [inverse_voc[w] if w in inverse_voc else inverse_voc['<below_th>'] for w in tokenize(line)]\n",
    "\n",
    "\n",
    "def fill_till_max(x,filler=-1,n=100):\n",
    "    \"\"\" Will make an array of fixed size n, will use x to fill this array. \n",
    "    If len(x) < n will fill the rest with filler. \n",
    "    \"\"\"\n",
    "    return [x[i] if len(x) > i else filler for i in range(n)]\n",
    "\n",
    "\n",
    "def map_seq_to_sentence(seq, voc):\n",
    "    \"\"\" Maps seqs back to a readable sentence\n",
    "    \"\"\"\n",
    "    return \" \".join([voc[int(w)] for w in seq]).replace(\" <eos>\",\".\")\n",
    "\n",
    "\n",
    "class TextGenerator():\n",
    "    \"\"\" Generates text from language model\n",
    "    \n",
    "    \"\"\"\n",
    "    TOP_N = 5\n",
    "\n",
    "    def __init__(self, lm, voc):\n",
    "        self.lm = lm\n",
    "        self.voc = voc\n",
    "        self.inv_voc = create_inverse_voc(voc)\n",
    "\n",
    "    def generate_text(self, seeds, max_len=100):\n",
    "        self.lm.reset_state()\n",
    "        # matrix of sentences in rows, words in columns\n",
    "        text_idx = np.array([fill_till_max(map_line_to_seq(s, self.inv_voc), n=max_len) for s in seeds], dtype=np.int32)\n",
    "\n",
    "        # i is the to predict word column\n",
    "        for i in range(2, max_len):\n",
    "            # Our input is all words before the one to predict\n",
    "            before = i - 1\n",
    "\n",
    "            # calculate probabilty\n",
    "            x = self.lm(text_idx[:, before])\n",
    "            d = softmax(x).data\n",
    "            next_words = []\n",
    "            d = np.delete(d, 0, 1)\n",
    "            for r in range(d.shape[0]):\n",
    "                top = nlargest(TextGenerator.TOP_N, enumerate(d[r, :]), itemgetter(1))\n",
    "                idx = [x[0] for x in top]\n",
    "                probs = [x[1] for x in top]\n",
    "                probs = np.array(probs) / np.sum(probs)\n",
    "                next_words.append(np.random.choice(idx, p=probs) + 1)\n",
    "\n",
    "            for si, w in enumerate(next_words):\n",
    "                if text_idx[si, i] < 0:\n",
    "                    text_idx[si, i] = w\n",
    "\n",
    "        return [map_seq_to_sentence(s, self.voc) for s in text_idx]\n",
    "\n",
    "\n",
    "print(\"Generating for english:\")\n",
    "seeds = [\"Sheep eat a lot of grass and produce wool. \", \"The Golden Rule or law of reciprocity is the principle of treating others as one would wish\"]\n",
    "rnn_en.predictor.train = False\n",
    "text = generate_text(rnn_en.predictor, voc_en, seeds)\n",
    "for t in text:\n",
    "    print(t)\n",
    "print(\"----\\n\")\n",
    "print(\"Generating for spanish:\")\n",
    "seeds = [\"La oveja (Ovis orientalis aries)1 es un mamífero cuadrúpedo ungulado doméstico, usado como ganado. Como todos los rumiantes, las ovejas son artiodáctilos, o animales con pezuñas.\", \"Regla de oro o ley de oro1 son denominaciones para un principio moral general que puede expresarse: trata a los demás como querrías que te trataran a ti (en su forma positiva)\"]\n",
    "rnn_es.predictor.train = False\n",
    "text = generate_text(rnn_es.predictor, voc_es, seeds)\n",
    "for t in text:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Analysing the word embeddings\n",
    "Similary to the work of mikolav, we want to see if it would be theoritacally possible to learn an linear embedding between the two languages. For this purpose we select the 10 most frequent words occuring in both languages, map the two languages to a 2d space using pca and plot them. For this purpose we make use of a modified version of: https://gist.github.com/chezou/3899461aa550f73854a1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 61678\n",
      "of 30433\n",
      "<eos> 28886\n",
      "in 27068\n",
      "and 25189\n",
      "a 20273\n",
      "to 17515\n",
      "was 11406\n",
      "is 10654\n",
      "ref> 10460\n",
      "on 7783\n",
      "for 7614\n",
      "by 6750\n",
      "as 6687\n",
      "he 6439\n",
      "s 6427\n",
      "with 6201\n",
      "at 5248\n",
      "that 5016\n",
      "his 5005\n",
      "from 4758\n",
      "it 4568\n",
      "<ref>< 4216\n",
      "an 3726\n",
      "name 3711\n",
      "<ref 2731\n",
      "are 2596\n",
      "were 2447\n",
      "also 2381\n",
      "which 2345\n",
      "< 2275\n",
      "has 2258\n",
      "this 2241\n",
      "first 2095\n",
      "2011 2033\n",
      "be 2032\n",
      "she 1896\n",
      "who 1892\n",
      "one 1858\n",
      "had 1779\n",
      "her 1779\n",
      "their 1679\n",
      "after 1663\n",
      "or 1662\n",
      "not 1624\n",
      "new 1584\n",
      "but 1575\n",
      "they 1561\n",
      "two 1498\n",
      "its 1469\n",
      ">< 1434\n",
      "series 1429\n",
      "have 1363\n",
      "1 1347\n",
      "been 1279\n",
      "all 1220\n",
      "may 1139\n",
      "2 1137\n",
      "time 1115\n",
      "during 1099\n",
      "when 1090\n",
      "other 1090\n",
      "film 1013\n",
      "there 1007\n",
      "year 1000\n",
      "world 976\n",
      "up 957\n",
      "season 937\n",
      "into 916\n",
      "three 903\n",
      "university 897\n",
      "de 893\n",
      "3 890\n",
      "born 877\n",
      "p 877\n",
      "where 872\n",
      "over 872\n",
      "him 868\n",
      "school 864\n",
      "later 849\n",
      "german 849\n",
      "more 839\n",
      "district 835\n",
      "national 833\n",
      "2012 827\n",
      "about 820\n",
      "i 814\n",
      "television 812\n",
      "only 807\n",
      "years 801\n",
      "made 790\n",
      "d 775\n",
      "september 774\n",
      "between 770\n",
      "city 764\n",
      "river 757\n",
      "second 749\n",
      "august 746\n",
      "m 743\n",
      "state 741\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter([s for s in seq_en if s != 0])\n",
    "for w,n in c.most_common(100):\n",
    "    print(voc_en[w],n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "given a word and visualize near words\n",
    "original source code is https://github.com/nishio/mycorpus/blob/master/vis.py\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "class visWord2Vec:\n",
    "    def __init__(self, data):\n",
    "        font = matplotlib.font_manager.FontProperties()\n",
    "        FONT_SIZE = 10\n",
    "        self.TEXT_KW = dict(fontsize=FONT_SIZE, fontweight='bold', fontproperties=font)\n",
    "        self.data = data\n",
    "        self.pca = pca\n",
    "\n",
    "    def plot(self, words,rotate=np.eye(2,2)):\n",
    "        \"\"\" Plots actual words of embedding, \n",
    "        :param words list of tuples (word_idx, word_txt)\n",
    "        \"\"\"\n",
    "        words_idx = [x[0] for x in words]\n",
    "        words_txt = [x[1] for x in words]\n",
    "        X = self.data[words_idx,:]\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X)\n",
    "        #print(pca.explained_variance_ratio_)\n",
    "        X = rotate * self.pca.transform(X) \n",
    "        print(X.shape)\n",
    "        xs = X[:, 0]\n",
    "        ys = X[:, 1]\n",
    "\n",
    "        # draw\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.scatter(xs, ys, marker = 'o')\n",
    "        for i, txt in enumerate(words_txt):\n",
    "            plt.annotate(\n",
    "                txt,\n",
    "                xy = (xs[i], ys[i]), xytext = (3, 3), textcoords = 'offset points', ha = 'left', va = 'top', **self.TEXT_KW)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1433, 'movie'), (665, 'city'), (1917, 'world'), (487, 'national'), (1887, 'university')]\n",
      "[(12976, 'pelicula(movie)'), (313, 'ciudad(city)'), (1943, 'mundo(world)'), (228, 'nacional(national)'), (190, 'universidad(university)')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,2) (5,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-aae5641ee0a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_es_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvis_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvis_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_en_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvis_es\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_es\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvis_es\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_es_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-2c94b8143e3d>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, words, rotate)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(pca.explained_variance_ratio_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrotate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,2) (5,2) "
     ]
    }
   ],
   "source": [
    "words_en = ['movie','city','world','national','university']\n",
    "words_es = ['pelicula','ciudad','mundo','nacional','universidad',]\n",
    "inv_voc_en = create_inverse_voc(voc_en)\n",
    "inv_voc_es = create_inverse_voc(voc_es)\n",
    "words_en_idx = [(inv_voc_en[w], w) for w in words_en]\n",
    "words_es_idx = [(inv_voc_es[w], \"%s(%s)\" % (w,w_en)) for w,w_en in zip(words_es,words_en)]\n",
    "print(words_en_idx)\n",
    "print(words_es_idx)\n",
    "vis_en = visWord2Vec(rnn_en.predictor.embed.W.data)\n",
    "vis_en.plot(words_en_idx)\n",
    "vis_es = visWord2Vec(rnn_es.predictor.embed.W.data)\n",
    "vis_es.plot(words_es_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
