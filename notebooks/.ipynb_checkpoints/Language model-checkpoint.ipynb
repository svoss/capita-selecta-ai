{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from wiki_dataset import get_wiki_dataset\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "In this notebook I will train the language models that I can use in a later stadium to try to learn our linear transformation layes. In this model we get as input a word embedding and as output the next word in the sequence. \n",
    "\n",
    "I will train a model for spanish and english."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Our model is based on the https://github.com/pfnet/chainer/tree/master/examples/ptb model. However we add an extra weights layer that can later be used to linearly transform the input matrix. In the beginning it will be an identity matrix and parameters will not be updated. In the next phase it will be the only one that is learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This a simple language model, directly copied from the tutorial\n",
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_units = n_units\n",
    "        self.n_vocab = n_vocab\n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        #for param in self.params():\n",
    "        #    param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h2 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RNNLM(86000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tutorial we re-use the following code:\n",
    "- **ParallelSequentialIteratior**: This class takes a dataset and creates batches(with batch size N) from it. In the first \\_\\_next()\\_\\_ step it will give the first words of N sequences as input and as target value the next word in the sequence. In the next \\_\\_next\\_\\_() call it will return the 2nd word of N sequences as input and the third word for as target etc. Running multiple sequences at the same time allows for efficient matrix multiplication(especially useful for GPU acceleration) furthermore the gradients for all sequences can be summed and applied immediately(allowing for statistic gradient descent). \n",
    "- **BPTTUpdater**: To obtain the weight gradients for our neural network we make use of back propagation trough time algorithm. It unfolds a neural network into a normal feedforward network where each layer represents one timestep, and then sums the gradients for each connection. The gradients are only saved for a fixed number of steps (\"truncated\"). \n",
    "- **Compute perplexity**: Perplexity is a measurement often used to evaluate language models in NLP context. It  assumes to obtain the softmax classification error as is used by default by the [chainer L.Classifier link](http://docs.chainer.org/en/stable/_modules/chainer/links/model/classifier.html?highlight=Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset iterator to create a batch of sequences at different positions.\n",
    "# This iterator returns a pair of current words and the next words. Each\n",
    "# example is a part of sequences starting from the different offsets\n",
    "# equally spaced within the whole sequence.\n",
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of ``__next__``.\n",
    "        self.iteration = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "\n",
    "\n",
    "# Custom updater for truncated BackProp Through Time (BPTT)\n",
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters\n",
    "\n",
    "\n",
    "# Routine to rewrite the result dictionary of LogReport to add perplexity\n",
    "# values\n",
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "First we have to divide our training, test and validation set. I use rougly the same fractions that are used in the get_ptb_words() dataset. First a 10% - 90% training - validation split. Then split the training again 10% - 90% to obtain a test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_and_split(dump):\n",
    "    seq, voc = get_wiki_dataset(dump)\n",
    "    seq = seq.astype(np.int32)\n",
    "    \n",
    "    val_start = int(len(seq) * .9)\n",
    "    test_start = int(val_start *.9)\n",
    "    train = seq[:test_start]\n",
    "    test = seq[test_start:val_start]\n",
    "    val = seq[val_start:]\n",
    "    \n",
    "    return train, val, test, voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://dumps.wikimeda.org/nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2...\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno socket error] [Errno 8] nodename nor servname provided, or not known",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ffe7cf5dd66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://dumps.wikimeda.org/nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dutch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-2ffe7cf5dd66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dump, name, test_mode, epoch, batch_size, gpu, out, grad_clip, brpoplen, resume)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mn_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#n_vocab=10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bee98dc38166>\u001b[0m in \u001b[0;36mretrieve_and_split\u001b[0;34m(dump)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wiki_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stijnvoss/Documents/UNI/capita-selecta-ai/code/wiki_dataset.pyc\u001b[0m in \u001b[0;36mget_wiki_dataset\u001b[0;34m(url, max)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svoss/chainer/wiki'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_or_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_build_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/chainer/dataset/download.pyc\u001b[0m in \u001b[0;36mcache_or_load_file\u001b[0;34m(path, creator, loader)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/stijnvoss/Documents/UNI/capita-selecta-ai/code/wiki_dataset.pyc\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdump_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/chainer/dataset/download.pyc\u001b[0m in \u001b[0;36mcached_download\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading from {}...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mopen_https\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrealhost\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Host'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0merrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmessage_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mmessage_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;31m#message_body was not a string (i.e. it is a file) and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m             \u001b[0mHTTPConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         self.sock = self._create_connection((self.host,self.port),\n\u001b[0;32m--> 836\u001b[0;31m                                            self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno socket error] [Errno 8] nodename nor servname provided, or not known"
     ]
    }
   ],
   "source": [
    "def train(dump, name, test_mode=False, epoch=5, batch_size=128, gpu=-1,out='result', grad_clip=True, brpoplen=35,resume=''):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    train, val, test,voc = retrieve_and_split(dump)\n",
    "    n_vocab = len(voc)\n",
    "    #n_vocab=10\n",
    "    print(\"Going to run %s\" % name) \n",
    "    print(\"#training: %d, #val: %d, #test: %d\" %(len(train), len(val), len(test)))\n",
    "    print(\"#vocabulary: %d\" % n_vocab)\n",
    "    \n",
    "    if test_mode:\n",
    "        print(\"Running in test mode, cutting test,train and val set to 100 elements each\")\n",
    "        train = train[:100]\n",
    "        test = test[:100]\n",
    "        val = val[:100]\n",
    "   \n",
    "    train_iter = ParallelSequentialIterator(train, batch_size)\n",
    "    val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "    test_iter = ParallelSequentialIterator(test, 1, repeat=False)\n",
    "\n",
    "    # Prepare an RNNLM model\n",
    "    print(\"Creating model\")\n",
    "    rnn = RNNLM(n_vocab, 800)\n",
    "    print(\"Init model complete\")\n",
    "    model = L.Classifier(rnn)\n",
    "    model.compute_accuracy = False  # we only want the perplexity\n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()  # make the GPU current\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Set up an optimizer\n",
    "    optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.GradientClipping(grad_clip))\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = BPTTUpdater(train_iter, optimizer, brpoplen, gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    eval_model = model.copy()  # Model with shared params and distinct states\n",
    "    eval_rnn = eval_model.predictor\n",
    "    eval_rnn.train = False\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "        val_iter, eval_model, device=gpu,\n",
    "        # Reset the RNN state at the beginning of each evaluation\n",
    "        eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "    interval = 10 if test_mode else 500\n",
    "    trainer.extend(extensions.LogReport(postprocess=compute_perplexity,\n",
    "                                        trigger=(interval, 'iteration')))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'iteration', 'perplexity', 'val_perplexity']\n",
    "    ), trigger=(interval, 'iteration'))\n",
    "    trainer.extend(extensions.ProgressBar(\n",
    "        update_interval=1 if test_mode else 10))\n",
    "    trainer.extend(extensions.snapshot())\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'model_iter_{.updater.iteration}'))\n",
    "    if resume:\n",
    "        chainer.serializers.load_npz(resume, trainer)\n",
    "\n",
    "    trainer.run()\n",
    "\n",
    "    # Evaluate the final model\n",
    "    print('test')\n",
    "    eval_rnn.reset_state()\n",
    "    evaluator = extensions.Evaluator(test_iter, eval_model, device=gpu)\n",
    "    result = evaluator()\n",
    "    print('test perplexity:', np.exp(float(result['main/loss'])))\n",
    "    \n",
    "    \n",
    "train('https://dumps.wikimedia.org/nlwiki/20161220/nlwiki-20161220-pages-articles1.xml.bz2', 'dutch', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
