{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Voynich manuscript using deep learning techniques\n",
    "\n",
    "The [voynisch manuscript](https://en.wikipedia.org/wiki/Voynich_manuscript) is a mysterical manuscript that has been the subject to studies for over 100 years but is still surrounded by a lot of questions and isn't translated until today. Theories include the manuscript being a haox, cipher or a natural language that does not exist anymore. \n",
    "\n",
    "For this research project I want to see if we can answers some of the questions that surround this manuscript using deep learning techniques or even can try to partly translate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start i decided to gather some textual data i could use, my idea is to look at properties over languages i can actually translate and understand to see what could work for the voynich manuscript. The voynich manuscript contains about 170,000 chars spread over 35,000 whitespaces seperated groups(probably words). Based on the illustrations it suspected that the following topics are covered:\n",
    "\n",
    "- Herbal (112 folios)\n",
    "- Astronomical (21 folios)\n",
    "- Biological (20 folios) \n",
    "- Cosmological (13 folios)\n",
    "- Pharmaceutical (34 folios)\n",
    "- Recipes (22 folios)\n",
    "\n",
    "My idea is to select pages from wikipedia that correlate to these topics from different languages and use the text on this pages as a corpus for our analysis. The fact that topics correlate means that it's more likely that the words in our dataset share sementatic meaning to the words in the manuscript. This definitely doesn't mean that exact translations will be available, and also note that the semenatic overlap between the different wikipedia datasets is likely to be higher than between the manuscript and the wikipedia pages. But i have to select some pages anyway, since we want an equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pharmaceutical chars found 26036 0\n",
      "Cosmological chars found 9954 0\n",
      "Astronomical chars found 16081 0\n",
      "Herbal chars found 85765 0\n",
      "Biological chars found 15315 0\n",
      "Recipes chars found 16846 0\n",
      "es total chars found 169997\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a2446d2962f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#find_titles_for_lang(['Geocentric model', 'Astronomical object','Planet','Heliocentrism','Galaxy'],'es')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0massemble_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0massemble_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0massemble_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0a2446d2962f>\u001b[0m in \u001b[0;36massemble_dataset\u001b[0;34m(lang, ds)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mds_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtopic_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmin_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_chars\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import codecs\n",
    "path = \"/Users/stijnvoss/Documents/uni/capita-selecta-ai/datasets/\"\n",
    "wm_titles = {\n",
    "    'Herbal': ['Vanilla','Mustard plant','Mustard seed','Pineapple','Pumpkin','Lime (fruit)','Parsley','Rosemary', 'Thymus (plant)', 'Apocynaceae', 'Basil','Oregano','Ballota_nigra','Ginger','Lavandula','Cumin','Nutmeg','Ruta graveolens','Anise','Peppermint','Mentha','Saffron','Achillea millefolium'],\n",
    "    'Astronomical': ['Geocentric model', 'Astronomical object','Planet','Heliocentrism','Galaxy'],\n",
    "    'Biological': ['Human body', 'Cardiology','Circulatory system'],\n",
    "    'Cosmological': ['Plato', 'God'],\n",
    "    'Pharmaceutical':['Antipyretic','Phytotherapy','Pharmaceutical_drug','Pharmacognosy'],\n",
    "    'Recipes':['Brussels sprout','Doberge cake','Angel cake','Gingerbread','Ontbijtkoek', \"Recipe\",\"Milk\", \"Risotto\", \"Paella\"]\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'Herbal': 112,\n",
    "    'Astronomical': 21,\n",
    "    'Biological': 20,\n",
    "    'Cosmological': 13,\n",
    "    'Pharmaceutical':34,\n",
    "    'Recipes':22\n",
    "}\n",
    "\n",
    "total_chars = 170000\n",
    "def find_titles_for_lang(titles, lang):\n",
    "    if lang == 'en':\n",
    "        return dict([(t,t) for t in titles])\n",
    "    else:\n",
    "        r = requests.get(\"https://en.wikipedia.org/w/api.php\",{\n",
    "                \"action\":'query',\n",
    "                'titles': \"|\".join(titles),\n",
    "                'prop':'langlinks',\n",
    "                'llinlanguagecode':'en',\n",
    "                'lllang':lang,\n",
    "                'lllimit':100,\n",
    "                'format':'json'})\n",
    "        return dict([(p['title'], p['langlinks'][0]['*']) for p in r.json()['query']['pages'].values() if 'langlinks' in p])\n",
    "\n",
    "def assemble_dataset(lang,ds):\n",
    "    global titles, weights, total_chars, path\n",
    "    total_weight = float(sum(weights.values()))\n",
    "    lang_folder = os.path.join(path,lang)\n",
    "    if not os.path.exists(lang_folder):\n",
    "        os.makedirs(lang_folder)\n",
    "    ds_chars = 0\n",
    "    for topic, titles in wm_titles.iteritems():\n",
    "        topic_chars = 0\n",
    "        min_chars = int(total_chars * (weights[topic]/total_weight))\n",
    "        translated_titles = find_titles_for_lang(titles, lang)\n",
    "        with codecs.open(os.path.join(lang_folder, topic +\".txt\"), 'w',encoding='utf8') as io:\n",
    "            for title in translated_titles.values():\n",
    "                chars, content = extract_content(title, lang)\n",
    "                for c in content:\n",
    "                    if c.strip() != '':\n",
    "                        min_chars = min_chars - 1\n",
    "                        topic_chars += 1\n",
    "                        ds_chars = ds_chars + 1\n",
    "                    if min_chars <= 0:\n",
    "                        break\n",
    "                    io.write(c)\n",
    "                if min_chars <= 0:\n",
    "                    break\n",
    "\n",
    "        print topic,\"chars found\", topic_chars, min_chars\n",
    "    print lang, \"total chars found\", ds_chars\n",
    "    \n",
    "def extract_content(wiki_title,lang):\n",
    "    wikipedia.set_lang(lang)\n",
    "    try:\n",
    "        p = wikipedia.page(wiki_title)\n",
    "        content = p.content.replace(\"=\",\"\")\n",
    "        return len(re.sub(r'\\s+', '', content)), content\n",
    "    except wikipedia.PageError as e:\n",
    "        return 0, \"\"\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        return 0, \"\"\n",
    "#find_titles_for_lang(['Geocentric model', 'Astronomical object','Planet','Heliocentrism','Galaxy'],'es')\n",
    "assemble_dataset('es', path)\n",
    "assemble_dataset('en', path)\n",
    "assemble_dataset('nl', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
