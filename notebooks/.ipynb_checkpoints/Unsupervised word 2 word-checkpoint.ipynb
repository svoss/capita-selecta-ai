{
 "cells": [
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised word 2 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
=======
   "cell_type": "code",
   "execution_count": 3,
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import numpy as np\n",
    "import os\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
=======
    "import chainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "First we create a model that contains an extra layer that is able to learn a translation matrix. We have to map from the old model to the new model."
=======
    "## Model"
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 2,
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This network builds a bit different network that add layers that we need to be able to learn our translation\n",
    "# First we need the translation layer \n",
    "# Secondly we need to the predict the next embedding instead of the word itself. If not we cannot re-use the network for the other language\n",
    "# We can however also no train the word embedding immid\n",
<<<<<<< HEAD
    "class TranslationRNN(chainer.Chain):\n",
    "\n",
    "    def __init__(self,n_units, n_vocab, train=True):\n",
    "        super(TranslationRNN, self).__init__(\n",
=======
    "class TransformingRNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, lm, train=True):\n",
    "        super(TransformingRNNLM, self).__init__(\n",
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l0=L.Linear(n_units,n_units,nobias=True),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
<<<<<<< HEAD
    "        self.n_vocab = n_vocab\n",
    "        self.n_units = n_units\n",
    "        \n",
=======
    "        \n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
    "        \n",
    "        # Our linear tranformation layer starts with \n",
    "        for param in self.l0.params():\n",
    "            param.data[...] = np.eye(n_units)\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l0(h0)\n",
    "        h2 = self.l1(F.dropout(h2, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h3, train=self.train))\n",
    "        y = self.l3(F.dropout(h2, train=self.train))\n",
<<<<<<< HEAD
    "        return y\n",
    "    \n",
    "class RNNLM(chainer.Chain):\n",
    "    def __init__(self, n_vocab, n_units, train=True):\n",
    "        super(RNNLM, self).__init__(\n",
    "            embed=L.EmbedID(n_vocab, n_units),\n",
    "            l1=L.LSTM(n_units, n_units),\n",
    "            l2=L.LSTM(n_units, n_units),\n",
    "            l3=L.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        self.n_units = n_units\n",
    "        self.n_vocab = n_vocab\n",
    "        # Initialize with uniform distribution, expect for our linear tranformation layer\n",
    "        # for param in self.params():\n",
    "        #    param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "\n",
    "        self.train = train\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h2 = self.l1(F.dropout(h0, train=self.train))\n",
    "        h3 = self.l2(F.dropout(h2, train=self.train))\n",
    "        y = self.l3(F.dropout(h3, train=self.train))\n",
=======
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
    "        return y"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load existing model and put it in the translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_rnn_model(f, n_vocab, n_units):\n",
    "    \"\"\"\n",
    "    Loads a simple pre-trained rnn language model in a classifier object\n",
    "    Exact training procedure is explained in language model notebook\n",
    "    \"\"\"\n",
    "    model = L.Classifier(RNNLM(n_vocab, n_units))\n",
    "    model.compute_accuracy = False\n",
    "    chainer.serializers.load_npz(f, model)\n",
    "    return model\n",
    "\n",
    "def transform_to_translation_model(flow_model, fit_model):\n",
    "    \"\"\"\n",
    "    Loads the weight parameters of two language models into an translation recurrent neural network\n",
    "    The flow_model is the model of which the weight of recurrent parts will be retrieved, this represent the flow in that language\n",
    "    The fit model is the language model of the to translate model, the embeded values and output layer will be used\n",
    "    \"\"\"\n",
    "    flow_rnn = flow_model.predictor\n",
    "    fit_rnn = fit_model.predictor\n",
    "    \n",
    "    TRNN = TranslationRNN(flow_rnn.n_units, fit_rnn.n_vocab)\n",
    "    \n",
    "    # Embed layer should be copied from the to fit layer\n",
    "    TRNN.embed.copyparams(fit_rnn.embed)\n",
    "    TRNN.embed.copyparams(fit_rnn.l3)\n",
    "    \n",
    "    # These are the recurrent parameters\n",
    "    TRNN.l1.copyparams(flow_rnn.l1)\n",
    "    TRNN.l1.copyparams(flow_rnn.l2)\n",
    "\n",
    "    #The l0 layer contains the translation matrix can't be copied \n",
    "    \n",
    "    return TRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENGLISH_VOC_SIZE = 8981\n",
    "ENGLISH_MODEL_FILE = '/Users/stijnvoss/Documents/UNI/capita-selecta-ai/result-english/model_iter_5430'\n",
    "SPANISH_VOC_SIZE = 8186\n",
    "SPANISH_MODEL_FILE = '/Users/stijnvoss/Documents/UNI/capita-selecta-ai/result-spanish/model_iter_5432'\n",
    "en_model = load_old_model(ENGLISH_MODEL_FILE, ENGLISH_VOC_SIZE, 800) \n",
    "es_model = load_old_model(SPANISH_MODEL_FILE, SPANISH_VOC_SIZE, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom optimizer to disable certain layer from being optimized \n",
    "We have to make sure that the gradients of the flow layer are not updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_model(TRNN):\n",
    "    return L.Classifier(TRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerSpecificGradientClearing(object):\n",
    "    \"\"\"Optimizer hook function for setting gradient to zero for certain layers in the network\n",
    "    \"\"\"\n",
    "    name = 'LayerSpecificGradientClearing'\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, opt):\n",
    "        xp = opt.target.xp\n",
    "        for param in opt.target.params():\n",
    "            grad = param.grad\n",
    "            with cuda.get_device(grad):\n",
    "                xp.clip(grad, self.lower_bound, self.upper_bound, out=grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks \n",
    "### When flow and fit model are the same model it should perform as this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updater really doesn't update unwanted weights"
   ]
  },
  {
=======
>>>>>>> 65fcefe791fd9a7edf7a8c3811e47dbeacc2e2d1
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
